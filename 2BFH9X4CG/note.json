{
  "paragraphs": [
    {
      "text": "%md \n\n# PDB Statistics\n\nfirst, let\u0027s load some data",
      "dateUpdated": "Feb 27, 2016 11:18:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456610723859_756353539",
      "id": "20160227-140523_23440935",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003ePDB Statistics\u003c/h1\u003e\n\u003cp\u003efirst, let\u0027s load some data\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 27, 2016 2:05:23 PM",
      "dateStarted": "Feb 27, 2016 11:18:56 PM",
      "dateFinished": "Feb 27, 2016 11:18:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val sqlContext \u003d new org.apache.spark.sql.SQLContext(sc)\n\nval ssummary \u003d sqlContext.read.json(\"/home/ubuntu/GIT/dataframes/json/v_structure_summary.2015_08_10.json\");\n\nssummary.first();\n\nssummary.registerTempTable(\"ssummary\")",
      "dateUpdated": "Feb 28, 2016 1:38:29 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456641188838_1814659028",
      "id": "20160227-223308_1525938884",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "sqlContext: org.apache.spark.sql.SQLContext \u003d org.apache.spark.sql.SQLContext@10e343b3\nssummary: org.apache.spark.sql.DataFrame \u003d [atom_site_count: bigint, classification: string, deposition_date: string, experimental_technique: string, macromolecule_type: string, ndb_id: string, pdb_doi: string, pdb_id: string, release_date: string, residue_count: bigint, resolution: double, revision_date: string, structure_molecular_weight: double, structure_title: string, v_structure_summary_ref_id: string]\nres166: org.apache.spark.sql.Row \u003d [0,DNA-RNA HYBRID,1994-12-05,X-RAY DIFFRACTION,DNA/RNA Hybrid,AHJ060,10.2210/pdb100d/pdb,100D,1995-03-31,20,1.9,2003-04-01#2009-02-24,6360.34,CRYSTAL STRUCTURE OF THE HIGHLY DISTORTED CHIMERIC DECAMER R(C)D(CGGCGCCG)R(G)-SPERMINE COMPLEX-SPERMINE BINDING TO PHOSPHATE ONLY AND MINOR GROOVE TERTIARY BASE-PAIRING,100D]\n"
      },
      "dateCreated": "Feb 27, 2016 10:33:08 PM",
      "dateStarted": "Feb 28, 2016 1:38:29 PM",
      "dateFinished": "Feb 28, 2016 1:38:36 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\n\nselect _c[10],_c[12]\nfrom ssummary\n\n",
      "dateUpdated": "Feb 28, 2016 1:46:08 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "_c0",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "_c1",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "_c0",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456695201203_1192694980",
      "id": "20160228-133321_2042932855",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.sql.AnalysisException: cannot recognize input near \u0027_c\u0027 \u0027[\u0027 \u002710\u0027 in expression specification; line 1 pos 7\n\tat org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:318)\n\tat org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)\n\tat org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:40)\n\tat scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)\n\tat scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)\n\tat scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)\n\tat scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)\n\tat scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)\n\tat scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)\n\tat org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)\n\tat org.apache.spark.sql.hive.HiveQl$.parseSql(HiveQl.scala:295)\n\tat org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)\n\tat org.apache.spark.sql.hive.HiveQLDialect$$anonfun$parse$1.apply(HiveContext.scala:66)\n\tat org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:279)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.liftedTree1$1(ClientWrapper.scala:226)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:225)\n\tat org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:268)\n\tat org.apache.spark.sql.hive.HiveQLDialect.parse(HiveContext.scala:65)\n\tat org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)\n\tat org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:211)\n\tat org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:114)\n\tat org.apache.spark.sql.execution.SparkSQLParser$$anonfun$org$apache$spark$sql$execution$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:113)\n\tat scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)\n\tat scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)\n\tat scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)\n\tat scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)\n\tat scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)\n\tat scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)\n\tat scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)\n\tat org.apache.spark.sql.catalyst.AbstractSparkSQLParser.parse(AbstractSparkSQLParser.scala:34)\n\tat org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)\n\tat org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:208)\n\tat org.apache.spark.sql.execution.datasources.DDLParser.parse(DDLParser.scala:43)\n\tat org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:231)\n\tat org.apache.spark.sql.hive.HiveContext.parseSql(HiveContext.scala:331)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:817)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:138)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n"
      },
      "dateCreated": "Feb 28, 2016 1:33:21 PM",
      "dateStarted": "Feb 28, 2016 1:46:08 PM",
      "dateFinished": "Feb 28, 2016 1:46:08 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456695292991_-702053791",
      "id": "20160228-133452_1505231123",
      "dateCreated": "Feb 28, 2016 1:34:52 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "PDB stats",
  "id": "2BFH9X4CG",
  "angularObjects": {
    "2BC4PK6ZQ": [],
    "2BC1EQWGY": [],
    "2BCM7EZYC": [],
    "2BFAHJ75U": [],
    "2BCDSU3BH": [],
    "2BFFJKBCS": [],
    "2BDR1NNQX": [],
    "2BBZ33TV9": [],
    "2BF9TS319": [],
    "2BBR4CWMF": [],
    "2BCTS3GY2": [],
    "2BE9P6DTD": [],
    "2BBTTBJK1": [],
    "2BDFU8JDA": []
  },
  "config": {},
  "info": {}
}