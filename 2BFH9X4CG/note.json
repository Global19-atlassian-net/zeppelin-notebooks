{
  "paragraphs": [
    {
      "text": "%md \n\n# PDB Statistics\n\nfirst, let\u0027s load some data",
      "dateUpdated": "Feb 27, 2016 11:18:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456610723859_756353539",
      "id": "20160227-140523_23440935",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003ePDB Statistics\u003c/h1\u003e\n\u003cp\u003efirst, let\u0027s load some data\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 27, 2016 2:05:23 PM",
      "dateStarted": "Feb 27, 2016 11:18:56 PM",
      "dateFinished": "Feb 27, 2016 11:18:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\n\n// Zeppelin creates and injects sc (SparkContext) and sqlContext (HiveContext or SqlContext)\n// So you don\u0027t need create them manually\n\n// load bank data\nval structText \u003d sc.parallelize(\n    IOUtils.toString(\n        new URL(\"https://github.com/rcsb/dataframes/blob/master/json/v_structure_summary.2015_08_10.json?raw\u003dtrue\"),\n        Charset.forName(\"utf8\")).split(\"\\n\"))\n        \n        \n ",
      "dateUpdated": "Feb 27, 2016 11:09:33 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456610786820_179458407",
      "id": "20160227-140626_2047485672",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\nstructText: org.apache.spark.rdd.RDD[String] \u003d ParallelCollectionRDD[135] at parallelize at \u003cconsole\u003e:61\n"
      },
      "dateCreated": "Feb 27, 2016 2:06:26 PM",
      "dateStarted": "Feb 27, 2016 11:09:33 PM",
      "dateFinished": "Feb 27, 2016 11:09:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\ncase class StrucSummary(\n    v_structure_summary_ref_id: String , \n    pdb_id: String,\n    structure_title: String, \n    experimentalTechnique: String, \n    ndb_id: String,\n    resolution: Float,\n    release_date: String,\n    deposition_date: String,\n    revision_date: String,\n    molecular_weight: Float,\n    macromolecule_type: String,\n    residue_count: Integer,\n    atom_site_count: Integer,\n    pdb_doi: String\n    \n)\n\nval ssummary \u003d structText.map(s \u003d\u003e s.split(\";\")).map(\n    s \u003d\u003e StrucSummary(\n            s(0).replaceAll(\"\\\"\", \"\"), \n            s(1).replaceAll(\"\\\"\", \"\"),\n            s(2).replaceAll(\"\\\"\", \"\"),\n            s(3).replaceAll(\"\\\"\", \"\"),\n            s(4).replaceAll(\"\\\"\", \"\"),\n            s(5).replaceAll(\"\\\"\", \"\").toFloat,\n            s(6).replaceAll(\"\\\"\", \"\"),\n            s(7).replaceAll(\"\\\"\", \"\"), \n            s(8).replaceAll(\"\\\"\", \"\"),\n            s(9).replaceAll(\"\\\"\", \"\").toFloat,        \n            s(10).replaceAll(\"\\\"\", \"\"),\n            s(11).replaceAll(\"\\\"\", \"\").toInt,\n            s(12).replaceAll(\"\\\"\", \"\").toInt,\n            s(13).replaceAll(\"\\\"\", \"\")\n        )\n).toDF()\nssummary.registerTempTable(\"ssummary\")\n",
      "dateUpdated": "Feb 27, 2016 11:23:04 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456640671105_156612909",
      "id": "20160227-222431_2071567119",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "defined class StrucSummary\nssummary: org.apache.spark.sql.DataFrame \u003d [v_structure_summary_ref_id: string, pdb_id: string, structure_title: string, experimentalTechnique: string, ndb_id: string, resolution: float, release_date: string, deposition_date: string, revision_date: string, molecular_weight: float, macromolecule_type: string, residue_count: int, atom_site_count: int, pdb_doi: string]\n"
      },
      "dateCreated": "Feb 27, 2016 10:24:31 PM",
      "dateStarted": "Feb 27, 2016 11:23:04 PM",
      "dateFinished": "Feb 27, 2016 11:23:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql \nselect experimentalTechnique\nfrom ssummary \ngroup by resolution \norder by resolution",
      "dateUpdated": "Feb 27, 2016 11:23:07 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456640938069_-1914310343",
      "id": "20160227-222858_2118189632",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.sql.AnalysisException: expression \u0027pdb_id\u0027 is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don\u0027t care which value you get.;\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:38)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:44)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:130)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$4.apply(CheckAnalysis.scala:158)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$4.apply(CheckAnalysis.scala:158)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:158)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:50)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:104)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:104)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:50)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:34)\n\tat org.apache.spark.sql.DataFrame.\u003cinit\u003e(DataFrame.scala:133)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:52)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:817)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:138)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n"
      },
      "dateCreated": "Feb 27, 2016 10:28:58 PM",
      "dateStarted": "Feb 27, 2016 11:21:03 PM",
      "dateFinished": "Feb 27, 2016 11:21:03 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Feb 27, 2016 10:34:49 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456641188838_1814659028",
      "id": "20160227-223308_1525938884",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Feb 27, 2016 10:33:08 PM",
      "dateStarted": "Feb 27, 2016 10:34:57 PM",
      "dateFinished": "Feb 27, 2016 10:34:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "PDB stats",
  "id": "2BFH9X4CG",
  "angularObjects": {
    "2BC4PK6ZQ": [],
    "2BC1EQWGY": [],
    "2BCM7EZYC": [],
    "2BFAHJ75U": [],
    "2BCDSU3BH": [],
    "2BFFJKBCS": [],
    "2BDR1NNQX": [],
    "2BBZ33TV9": [],
    "2BF9TS319": [],
    "2BBR4CWMF": [],
    "2BCTS3GY2": [],
    "2BE9P6DTD": [],
    "2BBTTBJK1": [],
    "2BDFU8JDA": []
  },
  "config": {},
  "info": {}
}